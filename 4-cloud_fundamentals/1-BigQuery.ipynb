{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86c034c-13c0-40b6-bd6a-c3c9d5c2a168",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Big Query\n",
    "- [Youtube Tutorial Video Link](https://www.youtube.com/watch?v=woU1YYlSR7o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84392a1e-aac9-43d7-9454-b58cd6129f03",
   "metadata": {},
   "source": [
    "## 1. Big Query concepts\n",
    "- Big Query is a service that allows you to query sql like queries against multiple terabytes of data at very high  speeds (seconds)\n",
    "- Big Query has a feature that is serverless (great for small teams)\n",
    "- Other data warehousing solutions include: amazon redshift and snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f14c61-bc2b-4094-806c-d3cc0831bb6d",
   "metadata": {},
   "source": [
    "suppose you are a data engineer and have a 5tb data you need to analyze:\n",
    "- what am i going to build with this data?\n",
    "- where is data comming from ?\n",
    "    - data collection method (scraping/ api/ buying from vendor/ generated by co)\n",
    "    - check the data governance and useage policy from docs or vendor\n",
    "- what type of storage to use and what system holds this data?\n",
    "    - local, on-premis cluster, cloud storage service (gcp)\n",
    "    - security, scalability\n",
    "    - reducndnecy and backups\n",
    "    - how is the data accessed\n",
    "- what does the data look like (data exploration)?\n",
    "    - structured or unstrutured\n",
    "    - format\n",
    "    - schema\n",
    "    - change over time\n",
    "    \n",
    "- what kind of analytics are going to be preformed?\n",
    "    - processing engine\n",
    "    - service availability\n",
    "    - analysis output\n",
    "        - who is going to access the data (data scientes/ analyst)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9b1ef-0989-48a5-84a5-7b9f2ee516fb",
   "metadata": {},
   "source": [
    "- `NOTE:` BIG DATA usally is referred to data > 1tb, because common RDBMS will start to degrate over time because they are not built for analystics (online transactional databases vs online analytical databases)\n",
    "    - OLAP: a software for performing multidimensional analysis at high speeds on large volumes of data from a data warehouse, data mart, or some other unified, centralized data store\n",
    "    - OLTP: is a type of data processing that consists of executing a number of transactions occurring concurrently, for example: online banking, shopping, order entry, or sending text messages.\n",
    "\n",
    "\n",
    "- when the RDBMS reach its limit its recommened to switch to a data warehouse system\n",
    "- when working with a mix of structured and unstructured data or just unstructured data a data lake system is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0adcc-9250-44b8-a9ea-dcb214fd26e0",
   "metadata": {},
   "source": [
    "Big tech companies using big query:\n",
    "- spotify\n",
    "     - user data (structured)\n",
    "     - audio data (unstructured)\n",
    "     - analytics for each user (big number of analytics processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb123c6-20de-4f87-b3eb-249694c85cf9",
   "metadata": {},
   "source": [
    "## 2. Interacting with Big Query:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ecd438-35d2-4ef8-b922-0b0178dcd574",
   "metadata": {},
   "source": [
    "### Login and User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67792bb-8171-40b9-9543-0ae8a74aea20",
   "metadata": {},
   "source": [
    " - login to a google account\n",
    " - use sandbox to avoid billing info\n",
    " - the top left product menu icon choose Big Query\n",
    " - in the Big Query menu you can choose from multiple sub-product for analysis, migration, and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad51ff-d864-467b-9d5d-86c83131b948",
   "metadata": {},
   "source": [
    "### Services\n",
    "- sql workspace: provide a code editor for sql queries \n",
    "- data transfer: an API service to import data into Big Query from various sources like:\n",
    "    - google saas data and google cloud storage(buckets),\n",
    "    - external storage providers: amazon S3, and azure blob storage \n",
    "    - external data warehouses: amazon redshift, tera data\n",
    "- schedual queries: automate queries\n",
    "- capacity manegment: specify a fixed amount of slots(pricing currency) for each job\n",
    "- BI Engine: optimize queries by using the most frequntly used data, including queries written by visulization tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab2ebb-27af-4510-83bf-ffb808ee06fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL workspace\n",
    "- using a publically available dataset on google cloud we can click the view button to open the dataset\n",
    "- in the work space resources section choose the data to work with (open the schema to explore metadata)\n",
    "- write a query to explore the data ex:\n",
    "```sql\n",
    "select country_region\n",
    "from `bigquery-public-data.covid19_jhu_csse.summary`\n",
    "group by 1\n",
    "limit 20;\n",
    "```\n",
    "    - note: the FROM table format is as follows: `project_name.dataset_name.table_name`\n",
    "    - the from is enclosed in `` not in '' or left without any\n",
    "- check the excution details on the query to understand the usage and the slot consubtion (price) of the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5ac0e-3db3-44ec-af7b-a0915053d82b",
   "metadata": {},
   "source": [
    "### Grants\n",
    "- grants permissions for users to preform specific actions ex: admin, data editor, data viewer\n",
    "- row base access is done using views (creating a query and saving it as av view to show only the date you want to show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8a4b3-158b-40c4-9953-f8de9465dc8b",
   "metadata": {},
   "source": [
    "### Slots\n",
    "A BigQuery slot is a virtual CPU used by BigQuery to execute SQL queries. During the query execution, BigQuery automatically calculates how many slots a query requires, depending on the query size and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5240a849-c178-479e-804c-30290342db4b",
   "metadata": {},
   "source": [
    "## 3.Loading data into Big Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7302a-68d2-4f18-a51c-eb739d2a6459",
   "metadata": {},
   "source": [
    "1. identify the data source and transform into one of the available data formats in bigquery:\n",
    "    - csv\n",
    "    - new_line_delimited_json\n",
    "    - ORC (Apache Hive) and PARQUET (Apache spark): Optimized Row Columnars, are columnar storage formats that are designed to improve performance for read-heavy workloads. great for fast query time and effiecent storage (used with big data solutions like apache products)\n",
    "    - AVRO(Hadoop): Avro is a compact and efficient data serialization format that supports schema evolution and self-description of data.\n",
    "    - datastore_backup: is a binary format used by Google Cloud Datastore to store backups of its NoSQL database. The format is optimized for efficient storage and retrieval of large amounts of data, and is designed to be scalable and reliable.\n",
    "\n",
    "2. define the data schema:\n",
    "- when defining a table schema each column should have the following defined: \n",
    "    - description:\n",
    "    - mode: Nullable, required, repeated \n",
    "    - type: DDL SQL data types (ex: int, float, numeric, etc)\n",
    "    - name: column name\n",
    "3. combine the data source and the schema into a table by creating a loading job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89cad8-7dc1-4074-a260-8c708a5ca896",
   "metadata": {
    "tags": []
   },
   "source": [
    "### loading data into BigQuery best practices\n",
    "- you have the option to load and store the data into BigQuery or read data from external data sources, but have worse preformance\n",
    "- when loading data into BigQuery load it incrementaly to catch any failures without losing all your progress\n",
    "- data storage on big query is cheaper long term compared to google cloud storage buckets\n",
    "- set table exporation date: Cost management, Performance optimization, Data governance\n",
    "- Documenting the access grants to external data sources :\n",
    "    - security and auditability:track users and apps and allow only authorized users to access the data\n",
    "    - complience: privacy regulations\n",
    "- take full advantage of nested and repeated feilds(cells that comtain multiple values)!!\n",
    "- when defining the schema manually skip the header row from the advanced options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa206981-83a0-4406-a8b4-cadfe042876d",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "1. example 1:\n",
    "- transform a dataset not compatible with big query using python\n",
    "- export the dataset into csv and upload using the big qurey UI\n",
    "\n",
    "2. example 2 (not yet implemented)\n",
    "- using gcp big query library upload the data in batches\n",
    "- create a dashboard that tracks:\n",
    "    - total uploaded data in gb\n",
    "    - total number of rows\n",
    "    - total number of nulls\n",
    "    - (batch no. or timestamp) vs stats as line graph  of the above stats, with a filter button for these stats \n",
    "\n",
    "__Example 1 datasets:__\n",
    "- [DATA SOURCE](https://iab.de/en/daten/iab-brain-drain/)\n",
    "\n",
    "- `Brain drain data`\n",
    "Contains data on the total number of foreign-born individuals aged 25 years and older living in each of the 20 considered OECD destination countries by year, gender, country of origin and educational level. Educational levels are distinguished into low, medium and high skilled.\n",
    "\n",
    "- `Migration by gender`\n",
    "Total number of foreign-born individuals (all age groups) living in each of the 20 considered OECD destination countries by gender and country of origin.\n",
    "\n",
    "- `Emigration rates`\n",
    "Proportion of migrants of the pre-migration population (defined as the sum of residents and migrants in each source country) by gender, skill level and year. Age group: 25 years and older.\n",
    "\n",
    "__Example 1 code:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe2e409-5892-4f5f-8cee-13fa29c4d6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url_brainDranin = 'https://doku.iab.de/daten/brain-drain/iabbd_8010_v1.dta'\n",
    "url_migrationByGender = 'https://doku.iab.de/daten/brain-drain/iabbd_8010_v1_gender.dta'\n",
    "url_emigration = 'https://doku.iab.de/daten/brain-drain/iabbd_8010_v1_emigration.dta'\n",
    "\n",
    "# load the data in chunks\n",
    "chunk_size = int(0.1 * 1024 * 1024) # 0.1 mb\n",
    "\n",
    "df_brainDrain_chunk = pd.read_stata(brainDranin_url,chunksize= chunk_size)\n",
    "df_brainDrain = pd.concat(df_brainDrain_chunk)\n",
    "\n",
    "df_migrationByGender_chunk= pd.read_stata(url_migrationByGender,chunksize= chunk_size)\n",
    "df_migrationByGender = pd.concat(df_migrationByGender_chunk)\n",
    "\n",
    "df_emigration_chunk = pd.read_stata(url_emigration,chunksize= chunk_size)\n",
    "df_emigration = pd.concat(df_emigration_chunk)\n",
    "\n",
    "df_migrationByGender.to_csv('C:\\\\Users\\\\mohimen\\\\Desktop\\\\mod_migrationByGender.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "762a5ceb-19d2-4298-97a6-f3841833d5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows: 53235 27293 4116\n",
      "3.35 MB\n",
      "1.41 MB\n",
      "0.2 MB\n"
     ]
    }
   ],
   "source": [
    "print('number of rows:',len(df_brainDrain), len(df_migrationByGender), len(df_emigration))\n",
    "# data size estimation\n",
    "print( (df_brainDrain.memory_usage().sum() / (1024*1024)).round(2), 'MB')\n",
    "print((df_migrationByGender.memory_usage().sum() / (1024*1024)).round(2), 'MB')\n",
    "print((df_emigration.memory_usage().sum() / (1024*1024)).round(2), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839aff8-d8a7-446a-8f6c-f30e981f1d0b",
   "metadata": {},
   "source": [
    "## 4. Data exploration in big query:\n",
    "- query features and best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462f031-a783-4843-b979-b17f2ac73211",
   "metadata": {},
   "source": [
    "### Query batching:\n",
    " - normal (interactive queries) run immediatlly without considration to available computing resources\n",
    " - batch query runs whenever there are free recourses available\n",
    " \n",
    "### Query results:\n",
    "- each query have the following results:\n",
    "    - job info tab: record data about the query such as query id, user id, creation timestamp, start time, duration, size,etc\n",
    "        - note: creation time and start time are not the same in batch queries since a query can wait for up to 24 hours to start running\n",
    "    - result and json tabs: results of the query in tablular format json format\n",
    "    - excution details: break down the excution operations (wait, read, compute, write) for the query operations (input , join, aggrigate, output, etc)\n",
    "\n",
    "### Query results are saved temporarly for 24h or can be saved:\n",
    "- locally: csv, json, clipboard (10mb)\n",
    "- on cloud: google sheets, csv(google drive), json(google drive) (1gb)\n",
    "- new permenant bigquery table\n",
    "\n",
    "### Scheduled queries (automated queries):\n",
    "- Scheduled a query to excute at a specified frequency (minute ,hourly, daily,weekly)\n",
    "- choose whether to overwrite or append the new data each time its quried\n",
    "- billing must be enabled\n",
    "\n",
    "### best practices for querying data on big query:\n",
    "- calculate and document your query cost by plugging the query cost into google priceing calculator (bigqeury)\n",
    "- avoid select * use preview instead\n",
    "- avoid user defined functions (extra computing)\n",
    "- materialize (save) large / complex query outputs into a table\n",
    "- in big query putting the smaller table on the left requires less computing\n",
    "- limit clause doesnt reduce the computing cost\n",
    "- avoid self joins\n",
    "- big query is not build for transactional or transformation opertaions:\n",
    "    - avoid insert delete and update table operations\n",
    "    - to manipulate your table you can pull and transform the data and reuplaod the table OR use a more advance engine like dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470c214-cb08-45b0-9b3a-c2b935a88464",
   "metadata": {},
   "source": [
    "## 5. Optmization\n",
    "- how big query work and how to optmize quries and reduce cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed3d6e-f377-463c-9a90-35d929dad1ac",
   "metadata": {},
   "source": [
    "### Schema design:\n",
    "\n",
    "- optimize for storage\n",
    "     - use normalized database schema\n",
    "         - group data into multiple tables and connect them through joins (less space more computation)\n",
    "         - reduce redundancy (avoids repeating columns and cells)\n",
    "- optmize for speed \n",
    "     - use denormalized database schema:\n",
    "         - group data into one table (no join logic to excute therefore faster excution)\n",
    "         - optmizes data appending\n",
    "         - use nested and repeated fields (ex:customer have multiple phone numbers)\n",
    "- general rule of thumb\n",
    "    - if a normalize table is less than 10GB its ok to leave it normalize there is no significant impact in the join logic. however, if there is alot of data manipulation (UPDATE, DELETE) operations denormalization is recommeneded\n",
    "    \n",
    "### table partitioning:\n",
    "group data based on values or range of values in a specified column and store each group separetly\n",
    "- partitioning tables:\n",
    "    - by time, date, or int\n",
    "- ingestion time partitioning table\n",
    "    - by ingestion or arrival date\n",
    "\n",
    "### clustring tables !!\n",
    "clustering groups the rows based on specified columns and then reorganize it based on how related each group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
