{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9df1646-c4e9-4db9-9fbd-0a3a04242ba6",
   "metadata": {},
   "source": [
    "## Introduction To big Data\n",
    "\n",
    "big data: \n",
    "- structured / unstructured / semi-structuered data\n",
    "- big data volumes processed contiously (stream / batch)\n",
    "- collected from multiple sources\n",
    "- data formats include: text, videos, images\n",
    "\n",
    "big data life cycle:\n",
    "1. business requirment.\n",
    "2. data collection.\n",
    "3. data modeling.\n",
    "4. data processing.\n",
    "5. data vizulization.\n",
    "\n",
    "big data 5 V:\n",
    "- velocity: rate of data transfer (batch , streaming)\n",
    "- volume: size of data (in petabytes or hexabytes)\n",
    "- varity: number of different data types\n",
    "- varcity: accuracy of data\n",
    "- value: the value produced from the big data cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686989e-5422-49ee-bacd-d5a9cb44ffd0",
   "metadata": {},
   "source": [
    "## Parallel Processing\n",
    "\n",
    "- how it works\n",
    "    - linear: solve problems by sequentially excuting instructions on a single processor\n",
    "    - parallel: excute instructions in parllel by using multiple processors for each non dependant instruction\n",
    "- error handeling\n",
    "    - linear: all instructions will be re-excuted after the error is solved\n",
    "    - parallel: only the instructions on the processor with the error will be re-excuted after the error is solved\n",
    "- scalability:\n",
    "    - linear: scales only vertically by upgrading the processing speed\n",
    "    - parallel: scales horizontally by adding more processors for more parallel processes\n",
    "- fault tolerance:    \n",
    "    - linear: if a node goes down the data is lost\n",
    "    - parallel: each partition in a node is backed in other nodes and if a node goes down the data can be recovered and used on a new node.\n",
    "        - note: backing up nodes is very complex so __Hadoop__ is used to mange the recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa838f-c43d-4594-ae56-03f224af8311",
   "metadata": {},
   "source": [
    "## Big Data Tools:\n",
    "\n",
    "|tool catigory|role|major vendors|\n",
    "|-------------|----|-------------|\n",
    "|data technology|analyze, process, extract information from big data handling structured and unstructered data and parallel processing the data at scale|hadoop, HDFS, spark, map_reduce|\n",
    "|visualization|analyze and visualize data|Tableau, SAS|\n",
    "|BI tools|transform data to insights|Power BI|\n",
    "|cloud providers| cloud storage |GCP, AWS, AZURE, Oracle|\n",
    "|NoSQL databases| NoSQL storage|MongoDB, Cassandra|\n",
    "|programming|Scripting|Python, Java, Scala|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc644ac-ce62-45fe-ac9b-b92a90b16960",
   "metadata": {},
   "source": [
    "## The Hadoop Ecosystem\n",
    "\n",
    "- __Hadoop Ingestion:__\n",
    "    - Flume is designed for streaming data ingestion\n",
    "    - Sqoop is designed for batch data ingestion, and it can be used to move data between relational databases and hadoop by generating map reduce code based on the database schema.\n",
    "\n",
    "\n",
    "- __Hadoop data storage:__\n",
    "    - Hadoop Distrbuted File System (HDFS): store data collected from ingestion and distrbute the data across multiple nodes\n",
    "    - HBase is a NoSQL, column-oriented database that is built on top of the Hadoop Distributed File System (HDFS). HBase is designed to store and manage big data in real-time.\n",
    "    - Cassandra is a scalable, NoSQL database designed to have no single point of failure.\n",
    "\n",
    "\n",
    "- __hadoop data analysis:__\n",
    "    - Pig: \n",
    "        - provides a SQL-like syntax that is translated to map reduce jobs.\n",
    "        - can be used to process data from various sources such as HDFS, HBase, or Amazon S3.\n",
    "        - Pig is suited for data processing tasks that involve data transformations, such as data cleaning, aggregation, or joining.\n",
    "    - Hive:\n",
    "        - a data warehousing tool that provides a SQL-like syntax  for querying and analyzing Hadoop datasets.\n",
    "        - Hive translates HiveQL queries into MapReduce jobs that are executed on the Hadoop cluster.\n",
    "        - It provides support for partitioning, bucketing, and indexing.\n",
    "\n",
    "\n",
    "- __data access:__\n",
    "    - hadoop user expieriance (HUE): allow users to uplad, browse, and query data\n",
    "    - impala: no code interface\n",
    "\n",
    "\n",
    "- __NOTES:__\n",
    "    - Older hadoop tools:\n",
    "        - Map reduce: Process big data by spiting data to smaller units to process requests in parallel\n",
    "        - Yet Another Recorse Negotiator (YARN): prepare Hadoop to batch, stream, interactive, and graph data processing\n",
    "\n",
    "    - Older hadoop tools Hadoop challenges:\n",
    "        - Hadoop was designed for only batch processing, and isn't well-suited for near-reeal/real-time data processing.\n",
    "        - it is not well-suited for more complex data processing tasks\n",
    "        - the overhead of setting up and managing a Hadoop cluster can outweigh the benefits of using the technology for smaller data sets.\n",
    "\n",
    "    - Solutions:\n",
    "        - Hive: built ontop of hadoop, and provids a SQL like query and statistical functions\n",
    "        - Pig: Multi-query approach (reduce number of times a query is scanned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292e8c0-bb0d-44b7-945b-47e4e1eaaddb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Map Reduce:\n",
    "\n",
    "- In a SQL query, the database is typically scanned sequentially, which can be slow and time-consuming for large datasets. In contrast, MapReduce processes data in parallel across a cluster of computers, which can be much faster for large datasets.\n",
    "\n",
    "- Another key difference is that SQL queries are typically used for structured data, while MapReduce is more flexible and can handle both structured and unstructured data. SQL queries are optimized for relational databases and can perform complex joins and aggregations on structured data. MapReduce, on the other hand, is designed to handle large, unstructured data sets, such as log files, social media data, and sensor data.\n",
    "\n",
    "- Additionally, MapReduce is more scalable than SQL, as it can handle petabytes of data across thousands of nodes in a distributed computing environment. SQL databases, on the other hand, can become slow and unwieldy as the data grows and the number of users accessing the database increases.\n",
    "\n",
    "- how does it work? :  \n",
    "    - split data to keys (ex: id) and values (ex: number sold, price) \n",
    "    - sort and aggrigate data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8568f461-3874-4df9-a188-414ff43a0e88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hadoop Distbuted File System (HDFS)\n",
    "\n",
    "- HDFS:\n",
    "    - splits files into blocks, create duplicates of the files, and store them on different machines\n",
    "    - provides access to streaming data\n",
    "    - uses a command line interface to interact with hadoop\n",
    "\n",
    "- key features:\n",
    "    - cost efficient and scalable: storage hardware is not expansive since it scales horizontally \n",
    "    - can store petabytes of data\n",
    "    - fault tolerant: copy data on multiple machines allowing data recovery in the event of a machine failure\n",
    "    - portability: can easily move across multiple platforms\n",
    "\n",
    "- HDFS concepts:\n",
    "    - blocks: \n",
    "        - the minimum amout of data that can be read or written with a default size of 64 or 128 MB\n",
    "        - ex: if a file size is 500 the result are 3 blocks of 128 MB and a block of 116 MB\n",
    "        - provide fault tolerance \n",
    "    - nodes: a single system that stores and process data\n",
    "        - Primary node: regulates file access to the client and maintains, maneges, and assign tasks to the secondary node\n",
    "        - secondary node: the main workers of the hdfs that takes instructions from the primary node\n",
    "    - rack awarness: \n",
    "        - a rack is a collection of 40 to 50 nodes using the same network switch\n",
    "        - maximize preformance by choosing data nodes racks that are closest to each other \n",
    "        - improve network preformace by reducing network traffic\n",
    "        - to achive rack awarness the name node(primary node) keep the rack id information\n",
    "        - replication is done to keep rack awarness in case of if a rack went down\n",
    "    - replication: hdfs optimzes replication using rack awerness for relaiable data and optmized network bandwidth utlization\n",
    "        - HDFS create duplicates for backup where the number of copies of each block is determined by the replication factor\n",
    "        - the duplicate blocks will be saved on different racks using the rack awarness\n",
    "    - read and write operations:\n",
    "        - hdfs allows write once (no edit) and read many\n",
    "        - read: \n",
    "            - the client will sends a requst to the primary node to get the location of the data\n",
    "            - the name node will varify that the client has the correct privilages and provide the location\n",
    "            - the client will interact with the primary and secondary nodes to fulfill the request\n",
    "            - the client will read the files closest to the data node\n",
    "        - write:\n",
    "            - the name node make sure that the file doesnt exist \n",
    "            - if the data exist an IO exception is recived\n",
    "            - if the data doesnt exist the client is given acces to start writing and backing up the files\n",
    "   - HDFS architcture:\n",
    "       - per cluster there is one name node and multiple data nodes (secondary nodes)\n",
    "       - the files are split into multiple blocks stored in multiple data nodes\n",
    "       - the name node oversees opening, closing, renaming file operations, and mapping file blocks to the data nodes\n",
    "       - the data nodes are responsible for read and write requests from the client and preform the create, replicate, delete instructions from the name node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0fd37-fef4-465c-8f96-59fad8beca7e",
   "metadata": {},
   "source": [
    "## Hive\n",
    "\n",
    "- Hive\n",
    "    - Hive is a data warehouse software within Hadoop that is designed to read, write, and manage large and tabular-type datasets and data analysis written in a sql like syntax hive query language(HiveQL).\n",
    "    - Hive allows for data cleaning and filtering tasks according to users' requirements\n",
    "    \n",
    "- RDBMS vs Hive\n",
    " |RDBMS|Hive|\n",
    " |-----|----|\n",
    " |maintain a database using SQL| maintain a datawarhouse using Hive Query Language|\n",
    " |suited for realtime dynamic data analysis like sensor data|suited for static data analysis like a text file|\n",
    " |handels data in terabytes|handels data in petabytes|\n",
    " |enforce the schema to varify loading data|doesnt enforce the schema to varify loading data|\n",
    " |may not have partitioning|have partitioning built in|\n",
    " \n",
    "- Hive architechture\n",
    "    - hive clients: provide different clients for communications with the servers\n",
    "        - ODBC: allow applications based on the ODBC protocol to connect to Hive\n",
    "        - JDBC: allow java applications to connect to hive\n",
    "    - hive services: preform the queries\n",
    "        - the Hive server runs queries and allow multiple clients to submit requests (ODB or JDBC)\n",
    "        - the driver recives query statments and submit it to the compiler \n",
    "        - the optmizer split the tasks\n",
    "        - the excuter excutes tasks after the optmizer has split the tasks\n",
    "        - the meta store stores meta data \n",
    "    - hive storage and computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76fddf-8f8d-4e0d-b6db-1491eb087022",
   "metadata": {},
   "source": [
    "## Hbase \n",
    "- a column orinted non-relaitonal database built on top of hadoop HDFS that provide \n",
    "- a fault toleratnt way of sorting sparse datasets, meaning it can work in unfavorable conditions such as when a server crashes\n",
    "- used for read heavy applications \n",
    "    - its linearly and modularly scalable (scales vertically and horizontally)\n",
    "    - its a backup support for map reduce jobs\n",
    "    - can be used for high speed requirments because it offers consistant reads and writes\n",
    "    - has no fixed column schema (works with column families)\n",
    "    - its has an easy to use java api access\n",
    "    - it provide data replication across clusters\n",
    "- __Example:__ for a table storing heart rate sensors data \n",
    "    - HBase allows many attributes(column) to be grouped into column families where all attributes of a column family are stored together for example the patiant name and age can be stored in a column family lets call it patiant details\n",
    "\n",
    "|Patiant Name|Patiant Age|Heart Rate|Time Stamp|\n",
    "|------------|-----------|----------|----------|\n",
    "|A|25|120 BPM|8:50 AM|\n",
    "|B|30|110 BPM|9:40 AM|\n",
    "|A|70|95 BPM|10:00 AM|\n",
    "|D|50|105 BPM|12:20 AM|\n",
    "\n",
    "- HBase requires to predefine the table shcema and column family\n",
    "- new column can be added to column families at any time\n",
    "- Hbase shcema is very flexable\n",
    "- Hbase hase master nodes to manage the clusters and region servers to preform the work\n",
    "\n",
    "- HBase vs HDFS:\n",
    "\n",
    "|HBase|HDFS|\n",
    "|------------|-----------|\n",
    "|Hbase store data in tabluar form|HDFS stores data in distrebuted manner across different nodes|\n",
    "|Hbase allow dynamic changes in the schema|HDFS has a ridged architecture that doesnt allow change|\n",
    "|Hbase allows random write and reads of data stored in HDFS|HDFS allows one write and many reads|\n",
    "|Hbase store data in tabluar form|HDFS stores data in distrebuted manner across different nodes|\n",
    "|HBase is for storage and processing|HDFS is for storage only|\n",
    "\n",
    "- HDFS components (ontop of hdfs)\n",
    "    - HMaster\n",
    "        - monitors reigon servers instances\n",
    "        - assign regions to reigon servers\n",
    "        - manges any changes made to the schema\n",
    "    - Region server\n",
    "        - recives and assign requsts for the reigons\n",
    "        - responible for maneging regions \n",
    "        - can communicate directly with the client \n",
    "    - HRegions\n",
    "        - smallest unit of HBase cluster\n",
    "        - contain multiple stores\n",
    "        - has two components HFile and memstore\n",
    "    - Zoo keeper\n",
    "        - maintains a health link between nodes \n",
    "        - provied distrbuted sync\n",
    "        - track server failure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc44a5-3886-456a-99b2-8429720b05cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
